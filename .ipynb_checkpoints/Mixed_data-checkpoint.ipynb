{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "533a65a6-652b-41f5-9d0f-784430d15daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# путь до данных на компьютере\n",
    "path = 'train_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa422261-bbfb-4188-9fda-68434d3d3dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "672fd357-6ca1-432f-9f7c-14faa47fd5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet_dataset_from_local(path_to_dataset: str, start_from: int = 0,\n",
    "                                     num_parts_to_read: int = 2, columns=None, verbose=False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    читает num_parts_to_read партиций, преобразовывает их к pd.DataFrame и возвращает\n",
    "    :param path_to_dataset: путь до директории с партициями\n",
    "    :param start_from: номер партиции, с которой нужно начать чтение\n",
    "    :param num_parts_to_read: количество партиций, которые требуется прочитать\n",
    "    :param columns: список колонок, которые нужно прочитать из партиции\n",
    "    :return: pd.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    res = []\n",
    "    dataset_paths = sorted([os.path.join(path_to_dataset, filename) for filename in os.listdir(path_to_dataset)\n",
    "                              if filename.startswith('train')])\n",
    "\n",
    "    start_from = max(0, start_from)\n",
    "    chunks = dataset_paths[start_from: start_from + num_parts_to_read]\n",
    "    if verbose:\n",
    "        print('Reading chunks:\\n')\n",
    "        for chunk in chunks:\n",
    "            print(chunk)\n",
    "    for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"):\n",
    "        print('chunk_path', chunk_path)\n",
    "        chunk = pd.read_parquet(chunk_path,columns=columns)\n",
    "        res.append(chunk)\n",
    "\n",
    "    return pd.concat(res).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2fc4c71-d4d3-4fcc-962f-518e5d0485de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_transactions_dataset(path_to_dataset: str, num_parts_to_preprocess_at_once: int = 1, num_parts_total: int=50,\n",
    "                                 save_to_path=None, verbose: bool=False):\n",
    "    \"\"\"\n",
    "    возвращает готовый pd.DataFrame с признаками, на которых можно учить модель для целевой задачи\n",
    "    path_to_dataset: str\n",
    "        путь до датасета с партициями\n",
    "    num_parts_to_preprocess_at_once: int\n",
    "        количество партиций, которые будут одновременно держаться и обрабатываться в памяти\n",
    "    num_parts_total: int\n",
    "        общее количество партиций, которые нужно обработать\n",
    "    save_to_path: str\n",
    "        путь до папки, в которой будет сохранён каждый обработанный блок в .parquet-формате; если None, то не будет сохранён\n",
    "    verbose: bool\n",
    "        логирует каждую обрабатываемую часть данных\n",
    "    \"\"\"\n",
    "    preprocessed_frames = []\n",
    "\n",
    "    for step in tqdm.tqdm_notebook(range(0, num_parts_total, num_parts_to_preprocess_at_once),\n",
    "                                   desc=\"Transforming transactions data\"):\n",
    "        transactions_frame = read_parquet_dataset_from_local(path_to_dataset, step, num_parts_to_preprocess_at_once,\n",
    "                                                             verbose=verbose)\n",
    "\n",
    "\n",
    "   #здесь должен быть препроцессинг данных\n",
    "        #выделим фичи для обучения\n",
    "#         columns_to_encode = transactions_frame.filter(regex='pre_loans\\d+|is_zero_loans\\d+').columns\n",
    "        columns_to_encode = transactions_frame.drop(['id', 'rn'], axis=1).columns\n",
    "        \n",
    "        #закодируем категории\n",
    "        ohe = OneHotEncoder(sparse_output=False, max_categories=14, drop='first')\n",
    "        encoded_frame = ohe.fit_transform(transactions_frame[columns_to_encode])\n",
    "        \n",
    "        #объединим закодированные категории с id\n",
    "        data_preprocessed = pd.concat([transactions_frame['id'], \n",
    "                                       pd.DataFrame(encoded_frame, columns=ohe.get_feature_names_out())], axis=1)\n",
    "        \n",
    "        #агрегируем по id: суммируем закодированные фичи по каждому клиенту\n",
    "        data_grouped = data_preprocessed.groupby('id', as_index=False).agg('sum')\n",
    "        \n",
    "        \n",
    "   #записываем подготовленные данные в файл\n",
    "        if save_to_path:\n",
    "            block_as_str = str(step)\n",
    "            if len(block_as_str) == 1:\n",
    "                block_as_str = '00' + block_as_str\n",
    "            else:\n",
    "                block_as_str = '0' + block_as_str\n",
    "            data_grouped.to_parquet(os.path.join(save_to_path, f'processed_chunk_{block_as_str}.parquet'))\n",
    "\n",
    "        preprocessed_frames.append(data_grouped)\n",
    "    \n",
    "    data = pd.concat(preprocessed_frames).fillna(0.)\n",
    "    \n",
    "    #смерджим по id с таргетом\n",
    "    targets = pd.read_csv('train_target/train_target.csv')\n",
    "    data = data.merge(targets, how='inner', on='id')\n",
    "    \n",
    "    #сохраним обработанный датафрейм\n",
    "    data.to_parquet('processed_data/data.pq', index=False)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72704f28-cbcc-4d6c-a775-763558b5dda3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c9f08bd8844c059d080fea9b92d2aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming transactions data:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading chunks:\n",
      "\n",
      "train_data/train_data_0.pq\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dfa8b3ea7c14c6fbf64d115b7d4d098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_0.pq\n",
      "Reading chunks:\n",
      "\n",
      "train_data/train_data_1.pq\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2545ad952e1f4e2192de94d4d294208d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_1.pq\n",
      "Reading chunks:\n",
      "\n",
      "train_data/train_data_10.pq\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "502e23f88bcd498dab32c52bf6ad9f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_10.pq\n",
      "Reading chunks:\n",
      "\n",
      "train_data/train_data_11.pq\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ddfe784b9f449889a92bd58624683c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_11.pq\n",
      "Reading chunks:\n",
      "\n",
      "train_data/train_data_2.pq\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52a8fdcdc084004aac3e3e65ec6a0a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_2.pq\n",
      "Reading chunks:\n",
      "\n",
      "train_data/train_data_3.pq\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0e05a81e8a94429af8b90d1aa710190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_3.pq\n",
      "Reading chunks:\n",
      "\n",
      "train_data/train_data_4.pq\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcbdfb44a5fb41c693bb5ae35366a679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_4.pq\n",
      "Reading chunks:\n",
      "\n",
      "train_data/train_data_5.pq\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c35584e7381c479fab2aa4224920b2d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_5.pq\n",
      "Reading chunks:\n",
      "\n",
      "train_data/train_data_6.pq\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a371f10f69dc4af7a4bdf3a2b09675ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_6.pq\n",
      "Reading chunks:\n",
      "\n",
      "train_data/train_data_7.pq\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e0898a44d843fa819a031a8c72c457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_7.pq\n",
      "Reading chunks:\n",
      "\n",
      "train_data/train_data_8.pq\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d544bef0df4bfea22a0b9067be43ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_8.pq\n",
      "Reading chunks:\n",
      "\n",
      "train_data/train_data_9.pq\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64eef261a1349bab6c4e2a90ab1ee16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_9.pq\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 7.67 GiB for an array with shape (343, 3000000) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_transactions_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_parts_to_preprocess_at_once\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_parts_total\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 52\u001b[0m, in \u001b[0;36mprepare_transactions_dataset\u001b[1;34m(path_to_dataset, num_parts_to_preprocess_at_once, num_parts_total, save_to_path, verbose)\u001b[0m\n\u001b[0;32m     48\u001b[0m         data_grouped\u001b[38;5;241m.\u001b[39mto_parquet(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_to_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_chunk_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblock_as_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     50\u001b[0m     preprocessed_frames\u001b[38;5;241m.\u001b[39mappend(data_grouped)\n\u001b[1;32m---> 52\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed_frames\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m#смерджим по id с таргетом\u001b[39;00m\n\u001b[0;32m     55\u001b[0m targets \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_target/train_target.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML_junior\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML_junior\\lib\\site-packages\\pandas\\core\\frame.py:5635\u001b[0m, in \u001b[0;36mDataFrame.fillna\u001b[1;34m(self, value, method, axis, inplace, limit, downcast)\u001b[0m\n\u001b[0;32m   5624\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   5625\u001b[0m \u001b[38;5;129m@doc\u001b[39m(NDFrame\u001b[38;5;241m.\u001b[39mfillna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_shared_doc_kwargs)\n\u001b[0;32m   5626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfillna\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5633\u001b[0m     downcast: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5634\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 5635\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5638\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5639\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdowncast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdowncast\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5642\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML_junior\\lib\\site-packages\\pandas\\core\\generic.py:6797\u001b[0m, in \u001b[0;36mNDFrame.fillna\u001b[1;34m(self, value, method, axis, inplace, limit, downcast)\u001b[0m\n\u001b[0;32m   6794\u001b[0m inplace \u001b[38;5;241m=\u001b[39m validate_bool_kwarg(inplace, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minplace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6795\u001b[0m value, method \u001b[38;5;241m=\u001b[39m validate_fillna_kwargs(value, method)\n\u001b[1;32m-> 6797\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6799\u001b[0m \u001b[38;5;66;03m# set the default here, so functions examining the signaure\u001b[39;00m\n\u001b[0;32m   6800\u001b[0m \u001b[38;5;66;03m# can detect if something was set (e.g. in groupby) (GH9221)\u001b[39;00m\n\u001b[0;32m   6801\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML_junior\\lib\\site-packages\\pandas\\core\\generic.py:5980\u001b[0m, in \u001b[0;36mNDFrame._consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   5977\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m():\n\u001b[0;32m   5978\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mconsolidate()\n\u001b[1;32m-> 5980\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_protect_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML_junior\\lib\\site-packages\\pandas\\core\\generic.py:5968\u001b[0m, in \u001b[0;36mNDFrame._protect_consolidate\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m   5966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f()\n\u001b[0;32m   5967\u001b[0m blocks_before \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mblocks)\n\u001b[1;32m-> 5968\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mblocks) \u001b[38;5;241m!=\u001b[39m blocks_before:\n\u001b[0;32m   5970\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML_junior\\lib\\site-packages\\pandas\\core\\generic.py:5978\u001b[0m, in \u001b[0;36mNDFrame._consolidate_inplace.<locals>.f\u001b[1;34m()\u001b[0m\n\u001b[0;32m   5977\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m():\n\u001b[1;32m-> 5978\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconsolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML_junior\\lib\\site-packages\\pandas\\core\\internals\\managers.py:686\u001b[0m, in \u001b[0;36mBaseBlockManager.consolidate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    684\u001b[0m bm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefs, verify_integrity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    685\u001b[0m bm\u001b[38;5;241m.\u001b[39m_is_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 686\u001b[0m \u001b[43mbm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bm\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML_junior\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1871\u001b[0m, in \u001b[0;36mBlockManager._consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_consolidated():\n\u001b[0;32m   1870\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1871\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m \u001b[43m_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1872\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1873\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefs \u001b[38;5;241m=\u001b[39m _consolidate_with_refs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML_junior\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2329\u001b[0m, in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   2327\u001b[0m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   2328\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[1;32m-> 2329\u001b[0m     merged_blocks, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_merge_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2330\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_consolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_can_consolidate\u001b[49m\n\u001b[0;32m   2331\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2332\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(merged_blocks, new_blocks)\n\u001b[0;32m   2333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML_junior\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2388\u001b[0m, in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[0;32m   2385\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m bvals2[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_concat_same_type(bvals2, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   2387\u001b[0m argsort \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(new_mgr_locs)\n\u001b[1;32m-> 2388\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mnew_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43margsort\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   2389\u001b[0m new_mgr_locs \u001b[38;5;241m=\u001b[39m new_mgr_locs[argsort]\n\u001b[0;32m   2391\u001b[0m bp \u001b[38;5;241m=\u001b[39m BlockPlacement(new_mgr_locs)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 7.67 GiB for an array with shape (343, 3000000) and data type float64"
     ]
    }
   ],
   "source": [
    "data = prepare_transactions_dataset(path, num_parts_to_preprocess_at_once=1, num_parts_total=12, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289fdbfa-7eba-4d5f-b47c-b56b9fd706c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X.astype(np.float32))\n",
    "        self.y = torch.from_numpy(y.astype(np.float32))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    \n",
    "class ClassificationNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden1 = nn.Linear(364, 256)\n",
    "        self.f1 = nn.Sigmoid()\n",
    "        self.hidden2 = nn.Linear(256, 10)\n",
    "        self.f2 = nn.Sigmoid()\n",
    "        self.output = nn.Linear(10, 1)\n",
    "        self.f3 = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.f1(self.hidden1(x))\n",
    "        x = self.f2(self.hidden2(x))\n",
    "        x = self.f3(self.output(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e5d072-1261-4bd2-bfdd-b5db0a8a898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#выбираем девайс на котором будет обучаться модель\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#задаем параметры\n",
    "num_epochs = 10\n",
    "batch_size = 1024\n",
    "learning_rate = 0.0001\n",
    "\n",
    "#объявляем модель перед загрузкой чанков\n",
    "model = ClassificationNet()\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "#объявляем оптимайзер\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#объявляем списки метрик\n",
    "loss_test = []\n",
    "roc_auc_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4753d66d-0416-48e9-95a9-2ce33e6a896b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#загружаем чанк\n",
    "print(f'Loading data: {chunk_path}')\n",
    "data = pd.read_parquet('preprocessed_data/data.pq')\n",
    "\n",
    "#разделяем на трейн и тест\n",
    "X = data.drop(['id', 'flag'], axis=1).to_numpy()\n",
    "y = data['flag'].to_numpy()\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, train_size=0.8, random_state=12, stratify=y)\n",
    "del data\n",
    "\n",
    "#считаем веса классов\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced', \n",
    "                                                  classes=np.unique(ytrain), \n",
    "                                                  y=ytrain)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "#объявляем датасет\n",
    "train_dataset = MyDataset(Xtrain, ytrain)\n",
    "Xtest_tensor = torch.from_numpy(Xtest.astype(np.float32)).to(device)\n",
    "ytest_tensor = torch.from_numpy(ytest.astype(np.float32)).to(device)\n",
    "\n",
    "#объявляем даталоадер\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in train_dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        weights = torch.zeros_like(y.unsqueeze(-1))\n",
    "        weights[y==0] = class_weights[0]\n",
    "        weights[y==1] = class_weights[1]\n",
    "        loss = F.binary_cross_entropy(pred, y.unsqueeze(-1), weight=weights)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        weights = torch.zeros_like(ytest_tensor.unsqueeze(-1))\n",
    "        weights[ytest_tensor==0] = class_weights[0]\n",
    "        weights[ytest_tensor==1] = class_weights[1]\n",
    "        loss = F.binary_cross_entropy(model(Xtest_tensor), ytest_tensor.unsqueeze(-1), weight=weights).item()\n",
    "        fpr, tpr, _ = roc_curve(ytest, model(Xtest_tensor).cpu().detach().numpy().ravel())\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        loss_test.append(loss)\n",
    "        roc_auc_test.append(roc_auc)\n",
    "        print(f'epoch {epoch} loss {loss} roc_auc {roc_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b979ae-e631-48a5-87b9-7c769112e1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = np.arange(0, num_epochs)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "plt.plot(step, np.array(loss_test))\n",
    "\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ce9575-3679-4ba7-b39a-3465721de434",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = np.arange(0, num_epochs)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "plt.plot(step, np.array(roc_auc_test))\n",
    "\n",
    "plt.title(\"ROC_AUC\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729c16d9-879f-49f3-973a-fa2de5dcd7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(ytest.to_numpy(), model(Xtest_tensor).cpu().detach().numpy().ravel())\n",
    "roc_auc = auc(fpr, tpr)\n",
    "RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc).plot()\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
